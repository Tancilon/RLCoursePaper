%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize, noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}
    \twocolumn[ \icmltitle{Submission and Formatting Instructions for \\ International Conference on Machine Learning (ICML 2025)}

    % It is OKAY to include author information, even for blind
    % submissions: the style file will automatically remove it for you
    % unless you've provided the [accepted] option to the icml2025
    % package.

    % List of affiliations: The first argument should be a (short)
    % identifier you will use later to specify author affiliations
    % Academic affiliations should list Department, University, City, Region, Country
    % Industry affiliations should list Company, City, Region, Country

    % You can specify symbols, otherwise they are numbered in order.
    % Ideally, you should not use this facility. Affiliations will be numbered
    % in order of appearance and this is the preferred way.
    \icmlsetsymbol{equal}{*}

    \begin{icmlauthorlist}
        \icmlauthor{Firstname1 Lastname1}{equal,yyy} \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
        \icmlauthor{Firstname3 Lastname3}{comp} \icmlauthor{Firstname4 Lastname4}{sch}
        \icmlauthor{Firstname5 Lastname5}{yyy} \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
        \icmlauthor{Firstname7 Lastname7}{comp}
        %\icmlauthor{}{sch}
        \icmlauthor{Firstname8 Lastname8}{sch} \icmlauthor{Firstname8 Lastname8}{yyy,comp}
        %\icmlauthor{}{sch}
        %\icmlauthor{}{sch}
    \end{icmlauthorlist}

    \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
    \icmlaffiliation{comp}{Company Name, Location, Country} \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

    \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu} \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

    % You may provide any keywords that you
    % find helpful for describing your paper; these are used to populate
    % the "keywords" metadata in the PDF but will not be shown in the document
    \icmlkeywords{Machine Learning, ICML}

    \vskip 0.3in ]

    % this must go after the closing bracket ] following \twocolumn[ ...

    % This command actually creates the footnote in the first column
    % listing the affiliations and the copyright notice.
    % The command takes one argument, which is text to display at the start of the footnote.
    % The \icmlEqualContribution command is standard text for equal contribution.
    % Remove it (just {}) if you do not need this facility.

    %\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
    \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

    \begin{abstract}
        This document provides a basic paper template and submission guidelines.
        Abstracts must be a single paragraph, ideally between 4--6 sentences
        long. Gross violations will trigger corrections at the camera-ready phase.
    \end{abstract}

    \section{Introduction}
    \label{intro}
    

    % \section{Electronic Submission}
    % \label{submission}

    % Submission to ICML 2025 will be entirely electronic, via a web site (not email).
    % Information about the submission process and \LaTeX\ templates are available
    % on the conference web site at:
    % \begin{center}
    %     \textbf{\texttt{http://icml.cc/}}
    % \end{center}

    % The guidelines below will be enforced for initial submissions and camera-ready
    % copies. Here is a brief summary:
    % \begin{itemize}
    %     \item Submissions must be in PDF\@.

    %     \item If your paper has appendices, submit the appendix together with
    %         the main body and the references \textbf{as a single file}.
    %         Reviewers will not look for appendices as a separate PDF file. So if
    %         you submit such an extra file, reviewers will very likely miss it.

    %     \item Page limit: The main body of the paper has to be fitted to 8 pages,
    %         excluding references and appendices; the space for the latter two is
    %         not limited in pages, but the total file size may not exceed 10MB.
    %         For the final version of the paper, authors can add one extra page to
    %         the main body.

    %     \item \textbf{Do not include author information or acknowledgements} in
    %         your initial submission.

    %     \item Your paper should be in \textbf{10 point Times font}.

    %     \item Make sure your PDF file only uses Type-1 fonts.

    %     \item Place figure captions \emph{under} the figure (and omit titles from
    %         inside the graphic file itself). Place table captions \emph{over}
    %         the table.

    %     \item References must include page numbers whenever possible and be as complete
    %         as possible. Place multiple citations in chronological order.

    %     \item Do not alter the style template; in particular, do not compress the
    %         paper format by reducing the vertical spaces.

    %     \item Keep your abstract brief and self-contained, one paragraph and
    %         roughly 4--6 sentences. Gross violations will require correction at
    %         the camera-ready phase. The title should have content words capitalized.
    % \end{itemize}

    \subsection{Deep Q-Network with Experience Replay}

    Q-learning is one of the earliest and most classical value-based methods in Reinforcement Learning, 
    originally proposed by Watkins and Dayan~\cite{watkins1992qlearning}. 
    It enables an agent to learn an optimal policy through trial-and-error interactions with the environment 
    without requiring prior knowledge of the environment’s transition probabilities. 
    he algorithm is formulated under the framework of a Markov Decision Process, 
    where decision-making scenarios are represented by states ($s$) and actions ($a$).
    Q-learning learns a state--action value function $Q(s, a)$, 
    which represents the expected long-term cumulative reward obtained by taking action $a$ in state $s$. 
    The learning process is governed by the \textbf{Bellman optimality equation}:
    \[
    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right],
    \]
    where $\alpha$ is the learning rate, $\gamma$ is the discount factor, and $r$ denotes the immediate reward. 

    The agent adopts an $\varepsilon$-greedy policy to balance \textit{exploration} (selecting random actions) 
    and \textit{exploitation} (choosing actions with the highest estimated value). 
    By iteratively updating the Q-values, the algorithm eventually converges to the optimal action--value function $Q^*(s, a)$. 
    The corresponding optimal policy is defined as
    \[
    \pi^*(s) = \arg\max_a Q^*(s, a).
    \]
    Q-learning is an \textit{off-policy} algorithm, meaning that it learns the optimal policy 
    while allowing the agent to explore the environment using any behavior policy.

    

    The following pseudocode gives a concise implementation flow for a Deep Q-Network
    (DQN) using an experience replay buffer and a target network. It is written
    in the standard \texttt{algorithm} / \texttt{algorithmic} environments and
    suitable as a high-level recipe for experiments.

    \begin{algorithm*}
        [tb]
        \caption{DQN with Experience Replay}
        \label{alg:dqn_replay}
        \begin{algorithmic}
            [1] \STATE {\bfseries Input:} environment Env, replay capacity $N$, batch
            size $B$, discount $\gamma$, target update period $C$, learning rate
            $\alpha$, exploration schedule $\epsilon_{t}$ \STATE Initialize replay
            buffer $\mathcal{D}\leftarrow \{\}$ of capacity $N$ \STATE Initialize
            online Q-network $Q(s,a;\theta)$ with random weights $\theta$ \STATE
            Initialize target network weights $\theta^{-}\leftarrow \theta$ \FOR{episode = 1 \textbf{to} M}
            \STATE $s \leftarrow$ Env.reset() \FOR{t = 1 \textbf{to} T} \STATE With
            probability $\epsilon_{t}$ select a random action $a$, otherwise
            $a \leftarrow \arg\max_{a'}Q(s,a';\theta)$ \STATE Execute $a$, observe
            reward $r$ and next state $s'$ \STATE Store transition $(s,a,r,s')$ in
            $\mathcal{D}$ (drop oldest if full) \IF{ $|\mathcal{D}| \ge B$ }
            \STATE Sample random batch
            $\{(s_{j},a_{j},r_{j},s'_{j})\}_{j=1}^{B}$ from $\mathcal{D}$ \STATE
            For each sample compute target:
            \[
                y_{j}=
                \begin{cases}
                    r_{j}                                          & \text{if }s'_{j}\text{ is terminal} \\
                    r_{j}+ \gamma \max_{a'}Q(s'_{j},a';\theta^{-}) & \text{otherwise}
                \end{cases}
            \]
            \STATE Perform a gradient step on loss $\mathcal{L}(\theta)=\frac{1}{B}
            \sum_{j}(y_{j}- Q(s_{j},a_{j};\theta))^{2}$ \ENDIF \STATE Every $C$ steps:
            $\theta^{-}\leftarrow \theta$ \STATE $s \leftarrow s'$ \ENDFOR
            \ENDFOR
        \end{algorithmic}
    \end{algorithm*}

    \paragraph{Notes.}
    Typical practical details: use RMSProp/Adam for optimization; clip rewards
    or normalize observations when appropriate; use a separate periodically-updated
    target network to stabilise learning; tune replay capacity $N$ and batch
    size $B$; optionally use prioritized replay or Double DQN extensions.

    \subsection{Double Deep Q-Network (Double DQN)}

    Double DQN reduces the overestimation bias of the original DQN by decoupling
    the action selection and action evaluation between the online network and
    the target network. The pseudocode below shows this modification to the target
    computation while keeping the overall training loop and replay buffer logic identical
    to standard DQN.

    \begin{algorithm*}
        [tb]
        \caption{Double DQN with Experience Replay}
        \label{alg:double_dqn}
        \begin{algorithmic}
            [1] \STATE {\bfseries Input:} environment Env, replay capacity $N$, batch
            size $B$, discount $\gamma$, target update period $C$, learning rate
            $\alpha$, exploration schedule $\epsilon_{t}$ \STATE Initialize replay
            buffer $\mathcal{D}\leftarrow \{\}$ of capacity $N$ \STATE Initialize
            online Q-network $Q(s,a;\theta)$ with random weights $\theta$ \STATE
            Initialize target network weights $\theta^{-}\leftarrow \theta$ \FOR{episode = 1 \textbf{to} M}
            \STATE $s \leftarrow$ Env.reset() \FOR{t = 1 \textbf{to} T} \STATE With
            probability $\epsilon_{t}$ select a random action $a$, otherwise
            $a \leftarrow \arg\max_{a'}Q(s,a';\theta)$ \STATE Execute $a$, observe
            reward $r$ and next state $s'$ \STATE Store transition $(s,a,r,s')$ in
            $\mathcal{D}$ (drop oldest if full) \IF{ $|\mathcal{D}| \ge B$ }
            \STATE Sample random batch
            $\{(s_{j},a_{j},r_{j},s'_{j})\}_{j=1}^{B}$ from $\mathcal{D}$ \STATE
            For each sample compute Double DQN target:
            \[
                y_{j}=
                \begin{cases}
                    r_{j}                                                                            & \text{if }s'_{j}\text{ is terminal} \\
                    r_{j}+ \gamma\; Q\bigl(s'_{j}, \arg\max_{a}Q(s'_{j},a;\theta) ; \theta^{-}\bigr) & \text{otherwise}
                \end{cases}
            \]
            \STATE Perform a gradient step on loss $\mathcal{L}(\theta)=\frac{1}{B}
            \sum_{j}(y_{j}- Q(s_{j},a_{j};\theta))^{2}$ \ENDIF \STATE Every $C$ steps:
            $\theta^{-}\leftarrow \theta$ \STATE $s \leftarrow s'$ \ENDFOR
            \ENDFOR
        \end{algorithmic}
    \end{algorithm*}

    \paragraph{Notes on Double DQN.}
    Compared to vanilla DQN, Double DQN uses the online network $\theta$ to
    select the argmax action and the target network $\theta^{-}$ to evaluate its
    value, which empirically reduces overestimation and often improves stability.
    Other recommended improvements (e.g., prioritized replay, dueling
    architecture, n-step returns, Huber loss) are compatible with this change.

    \subsection{Dueling Deep Q-Network (Dueling DQN)}

    Dueling DQN separates the representation of state-value and advantage for
    each action, which helps the agent learn which states are (or are not) valuable
    without having to learn the effect of each action for every state. The
    architecture splits the final layers of the Q-network into two streams that
    estimate a scalar state-value $V(s)$ and an advantage vector $A(s,a)$; these
    are combined to produce Q-values via $Q(s,a)=V(s) + A(s,a) - \frac{1}{|\mathcal{A}|}
    \sum_{a'}A(s,a')$.

    The pseudocode below shows a dueling variant that uses experience replay and
    a target network; it otherwise follows the same training loop as the DQN
    family (optionally usable together with Double DQN selection/evaluation).

    \begin{algorithm*}
        [tb]
        \caption{Dueling DQN with Experience Replay}
        \label{alg:dueling_dqn}
        \begin{algorithmic}
            [1] \STATE {\bfseries Input:} environment Env, replay capacity $N$, batch
            size $B$, discount $\gamma$, target update period $C$, learning rate
            $\alpha$, exploration schedule $\epsilon_{t}$ \STATE Initialize replay
            buffer $\mathcal{D}\leftarrow \{\}$ of capacity $N$ \STATE Initialize
            online dueling Q-network: shared trunk; value head $V(s;\theta)$;
            advantage head $A(s,a;\theta)$; combine to $Q(s,a;\theta)$ via advantage
            normalization \STATE Initialize target network weights $\theta^{-}\leftarrow
            \theta$ \FOR{episode = 1 \textbf{to} M} \STATE $s \leftarrow$ Env.reset()
            \FOR{t = 1 \textbf{to} T} \STATE With probability $\epsilon_{t}$ select
            a random action $a$, otherwise
            $a \leftarrow \arg\max_{a'}Q(s,a';\theta)$ \STATE Execute $a$, observe
            reward $r$ and next state $s'$ \STATE Store transition $(s,a,r,s')$ in
            $\mathcal{D}$ (drop oldest if full) \IF{ $|\mathcal{D}| \ge B$ }
            \STATE Sample random batch
            $\{(s_{j},a_{j},r_{j},s'_{j})\}_{j=1}^{B}$ from $\mathcal{D}$ \STATE
            For each sample compute target $y_{j}$ (use either DQN or Double DQN
            style target):
            \[
                y_{j} =
                \begin{cases}
                    r_{j}                                           & \text{if }s'_{j} \text{ is terminal} \\
                    r_{j} + \gamma \max_{a'}Q(s'_{j},a';\theta^{-}) & \text{otherwise}
                \end{cases}
            \]
            \STATE Note: when combining with Double DQN, replace the $\max$ evaluation
            by
            \[
                r_{j} + \gamma\; Q\bigl(s'_{j}, \arg\max_{a} Q(s'_{j},a;\theta) ;
                \theta^{-} \bigr)
            \]
            \STATE Perform a gradient step on loss $\mathcal{L}(\theta)=\frac{1}{B}
            \sum_{j} (y_{j} - Q(s_{j},a_{j};\theta))^{2}$; gradients
            backpropagate through both value and advantage heads \ENDIF \STATE
            Every $C$ steps: $\theta^{-} \leftarrow \theta$ \STATE $s \leftarrow
            s'$ \ENDFOR \ENDFOR
        \end{algorithmic}
    \end{algorithm*}

    \paragraph{Notes on Dueling DQN.}
    The dueling architecture often speeds learning of state values and leads to
    more stable policies, particularly in environments where many actions have similar
    effects. Ensure advantage normalization (subtracting the mean advantage)
    when reconstructing Q-values to guarantee identifiability of $V$ and $A$. Dueling
    is orthogonal to other improvements (Double DQN, prioritized replay, n-step
    returns) and can be combined with them.


    \section{Experimental Setup}

    \subsection{Hardware and System}
    All experiments were executed on a single workstation with the following configuration:
    \begin{itemize}
        \item \textbf{CPU:} Intel Xeon Gold 6326 @ 2.90 GHz
        \item \textbf{GPU:} NVIDIA RTX A6000
        \item \textbf{OS:} Ubuntu 20.04.4 LTS
    \end{itemize}
    No other concurrent training jobs were scheduled on the GPU during runs.

    \subsection{Task Environments}
    We evaluate on two canonical control benchmarks using the Gymnasium API with ale-py:
    \paragraph{CartPole-v3.}
    A classic control task with a 4-dimensional continuous observation vector and a discrete 2-action space (push left/right). Episodes terminate or truncate according to the environment’s default criteria; all default physics and termination settings are retained.
    \paragraph{LunarLander-v3 (discrete).}
    A 2-D lander with an 8-dimensional continuous observation vector and a discrete 4-action space (do nothing; left/right engine; main engine). Episodes terminate upon crash or successful landing; time-limit truncation follows the environment’s default. We keep all default reward shaping and environment parameters in both tasks.

    \subsection{Training Configuration}
    We train value-based agents from the DQN family with standard experience replay and a target network. For the loss, we adopt the Huber loss in all settings, as it is empirically more robust to outliers and stabilizes Q-learning updates relative to mean-squared error. To ensure reproducibility and comparability, hyperparameters follow widely used community baselines, varying only where task-specific totals or memory/mini-batch sizes are customary. Exploration is $\varepsilon$-greedy with linear decay from the initial to the final value over the specified exploration fraction of total training steps.

    \begin{table}[t]
        \centering
        \caption{Training hyperparameters. CartPole-v3 and LunarLander-v3 (discrete) follow community baseline configurations; Huber loss is used throughout.}
        \label{tab:hyperparams}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{@{}lcc@{}}
            \toprule
            \textbf{Hyperparameter} & \textbf{CartPole-v3} & \textbf{LunarLander-v3 (discrete)} \\
            \midrule
            \texttt{total\_timesteps}          & 100000 & 500000 \\
            \texttt{learning\_rate}            & $5\times 10^{-4}$ & $5\times 10^{-4}$ \\
            \texttt{buffer\_size}              & 50000  & 200000 \\
            \texttt{batch\_size}               & 64     & 128 \\
            \texttt{gamma}                     & 0.99   & 0.99 \\
            \texttt{train\_frequency}          & 1      & 4 \\
            \texttt{gradient\_steps}           & 1      & 1 \\
            \texttt{target\_update\_interval}  & 1000   & 4000 \\
            \texttt{target\_update\_tau}       & 1.0    & 1.0 \\
            \texttt{learning\_starts}          & 1000   & 10000 \\
            \texttt{exploration\_fraction}     & 0.20   & 0.40 \\
            \texttt{exploration\_initial\_eps} & 1.00   & 1.00 \\
            \texttt{exploration\_final\_eps}   & 0.05   & 0.02 \\
            \texttt{loss}                      & Huber  & Huber \\
            \bottomrule
        \end{tabular}}
    \end{table}

    \subsection{Evaluation Protocol}
    We report learning curves as the episodic return obtained from deterministic evaluation rollouts ($\epsilon = 0$) at fixed training intervals. At each interval, we execute an evaluation rollout with the current policy and record its episodic return; the curve is the sequence of these values over training. No smoothing is applied and all results are from a single random seed unless stated otherwise.

    \section{Results}
    Figure~\ref{fig:learning_curves} shows the learning curve across both tasks. Under identical hyperparameters, our runs exhibit a consistent ranking $\text{DQN} < \text{Double-DQN} < \text{Dueling-DQN}$: DQN improves but displays noticeable oscillations and occasional regressions; Double-DQN dampens these fluctuations and is more sample-efficient; Dueling-DQN accelerates early learning and achieves the highest plateau with the most stable trajectory. The gap is most pronounced in the harder phases of training, where the variants sustain steadier progress toward convergence. For qualitative results (videos) from the optimized models for each algorithm and environment, please refer to the project repository: https://github.com/Hyrsta/Q-learning
    
    \section{Conclusion}
    This study evaluated value-based deep reinforcement learning algorithms; DQN, Double-DQN, and Dueling-DQN on CartPole-v3 and LunarLander-v3 (discrete) under matched hyperparameters and a common training protocol. Across both tasks, the results consistently rank $\text{DQN} < \text{Double-DQN} < \text{Dueling-DQN}$ in terms of stability, sample efficiency, and attained return. These outcomes align with established explanations: Double-DQN mitigates overestimation bias, while the dueling architecture separates state value and advantage to improve value estimation efficiency. Using a unified setup (identical optimizers, Huber loss, and community-baseline hyperparameters) strengthens comparability and offers a clean empirical signal for the incremental benefits of these design choices.

    \begin{figure*}[th]
        \centering
        % Compiles even if the file doesn't exist; replace with your actual figure later.
        \IfFileExists{figures/learning_curves.png}{%
            \includegraphics[width=0.85\linewidth]{figures/learning_curves.png}%
        }{%
            \fbox{\parbox{0.85\linewidth}{\centering Placeholder: learning\_curves.pdf not found.\\
            Put your plot at \texttt{figures/learning\_curves.pdf}.}}%
        }
        \caption{Learning curves on CartPole-v3 and LunarLander-v3 (discrete). Shaded regions indicate variability across random seeds.}
        \label{fig:learning_curves}
    \end{figure*}

%     \subsection{Submitting Papers}

%     \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
%     author information may appear on the title page or in the paper itself. \cref{author
%     info} gives further details.

%     \medskip

%     Authors must provide their manuscripts in \textbf{PDF} format. Furthermore, please
%     make sure that files contain only embedded Type-1 fonts (e.g.,~using the
%     program \texttt{pdffonts} in linux or using File/DocumentProperties/Fonts in
%     Acrobat). Other fonts (like Type-3) might come from graphics files imported into
%     the document.

%     Authors using \textbf{Word} must convert their document to PDF\@. Most of
%     the latest versions of Word have the facility to do this automatically. Submissions
%     will not be accepted in Word format or any format other than PDF\@. Really.
%     We're not joking. Don't send Word.

%     Those who use \textbf{\LaTeX} should avoid including Type-3 fonts. Those using
%     \texttt{latex} and \texttt{dvips} may need the following two commands:

%     {\footnotesize \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
%     It is a zero following the ``-G'', which tells dvips to use the config.pdf file.
%     Newer \TeX\ distributions don't always need this option.

%     Using \texttt{pdflatex} rather than \texttt{latex}, often gives better results.
%     This program avoids the Type-3 font problem, and supports more advanced features
%     in the \texttt{microtype} package.

%     \textbf{Graphics files} should be a reasonable size, and included from an
%     appropriate format. Use vector formats (.eps/.pdf) for plots, lossless
%     bitmap formats (.png) for raster graphics with sharp lines, and jpeg for
%     photo-like images.

%     The style file uses the \texttt{hyperref} package to make clickable links in
%     documents. If this causes problems for you, add \texttt{nohyperref} as one
%     of the options to the \texttt{icml2025} usepackage statement.

%     \subsection{Submitting Final Camera-Ready Copy}

%     The final versions of papers accepted for publication should follow the same
%     format and naming convention as initial submissions, except that author information
%     (names and affiliations) should be given. See \cref{final author} for
%     formatting instructions.

%     The footnote, ``Preliminary work. Under review by the International
%     Conference on Machine Learning (ICML). Do not distribute.'' must be modified
%     to ``\textit{Proceedings of the $\mathit{42}^{nd}$ International Conference
%     on Machine Learning}, Vancouver, Canada, PMLR 267, 2025. Copyright 2025 by the
%     author(s).''

%     For those using the \textbf{\LaTeX} style file, this change (and others) is
%     handled automatically by simply changing
%     $\mathtt{\backslash usepackage\{icml2025\}}$ to
%     \[
%         \mathtt{\backslash usepackage[accepted]\{icml2025\}}
%     \]
%     Authors using \textbf{Word} must edit the footnote on the first page of the document
%     themselves.

%     Camera-ready copies should have the title of the paper as running head on each
%     page except the first one. The running title consists of a single line
%     centered above a horizontal rule which is $1$~point thick. The running head should
%     be centered, bold and in $9$~point type. The rule should be $10$~points above
%     the main text. For those using the \textbf{\LaTeX} style file, the original
%     title is automatically set as running head using the \texttt{fancyhdr} package
%     which is included in the ICML 2025 style file package. In case that the
%     original title exceeds the size restrictions, a shorter form can be supplied
%     by using

%     \verb|\icmltitlerunning{...}|

%     just before $\mathtt{\backslash begin\{document\}}$. Authors using \textbf{Word}
%     must edit the header of the document themselves.

%     \section{Format of the Paper}

%     All submissions must follow the specified format.

%     \subsection{Dimensions}

%     The text of the paper should be formatted in two columns, with an overall
%     width of 6.75~inches, height of 9.0~inches, and 0.25~inches between the
%     columns. The left margin should be 0.75~inches and the top margin 1.0~inch (2.54~cm).
%     The right and bottom margins will depend on whether you print on US letter
%     or A4 paper, but all final versions must be produced for US letter size. Do
%     not write anything on the margins.

%     The paper body should be set in 10~point type with a vertical spacing of 11~points.
%     Please use Times typeface throughout the text.

%     \subsection{Title}

%     The paper title should be set in 14~point bold type and centered between two
%     horizontal rules that are 1~point thick, with 1.0~inch between the top rule and
%     the top edge of the page. Capitalize the first letter of content words and
%     put the rest of the title in lower case.

%     \subsection{Author Information for Submission}
%     \label{author info}

%     ICML uses double-blind review, so author information must not appear. If you
%     are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
%     \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+
%     to specify affiliations. (Read the TeX code used to produce this document
%     for an example usage.) The author information will not be printed unless
%     \texttt{accepted} is passed as an argument to the style file. Submissions
%     that include the author information will not be reviewed.

%     \subsubsection{Self-Citations}

%     If you are citing published papers for which you are an author, refer to
%     yourself in the third person. In particular, do not use phrases that reveal
%     your identity (e.g., ``in previous work \cite{langley00}, we have shown
%     \ldots'').

%     Do not anonymize citations in the reference section. The only exception are
%     manuscripts that are not yet published (e.g., under submission). If you choose
%     to refer to such unpublished manuscripts \cite{anonymous}, anonymized copies
%     have to be submitted as Supplementary Material via OpenReview\@. However, keep
%     in mind that an ICML paper should be self contained and should contain sufficient
%     detail for the reviewers to evaluate the work. In particular, reviewers are not
%     required to look at the Supplementary Material when writing their review (they
%     are not required to look at more than the first $8$ pages of the submitted
%     document).

%     \subsubsection{Camera-Ready Author Information}
%     \label{final author}

%     If a paper is accepted, a final camera-ready copy must be prepared.
%     %
%     For camera-ready papers, author information should start 0.3~inches below the
%     bottom rule surrounding the title. The authors' names should appear in 10~point
%     bold type, in a row, separated by white space, and centered. Author names should
%     not be broken across lines. Unbolded superscripted numbers, starting 1,
%     should be used to refer to affiliations.

%     Affiliations should be numbered in the order of appearance. A single footnote
%     block of text should be used to list all the affiliations. (Academic affiliations
%     should list Department, University, City, State/Region, Country. Similarly
%     for industrial affiliations.)

%     Each distinct affiliations should be listed once. If an author has multiple
%     affiliations, multiple superscripts should be placed after the name,
%     separated by thin spaces. If the authors would like to highlight equal contribution
%     by multiple first authors, those authors should have an asterisk placed
%     after their name in superscript, and the term ``\textsuperscript{*}Equal contribution"
%     should be placed in the footnote block ahead of the list of affiliations. A list
%     of corresponding authors and their emails (in the format Full Name \textless{}email@domain.com\textgreater{})
%     can follow the list of affiliations. Ideally only one or two names should be
%     listed.

%     A sample file with author names is included in the ICML2025 style file
%     package. Turn on the \texttt{[accepted]} option to the stylefile to see the
%     names rendered. All of the guidelines above are implemented by the \LaTeX\ style
%     file.

%     \subsection{Abstract}

%     The paper abstract should begin in the left column, 0.4~inches below the final
%     address. The heading `Abstract' should be centered, bold, and in 11~point
%     type. The abstract body should use 10~point type, with a vertical spacing of
%     11~points, and should be indented 0.25~inches more than normal on left-hand and
%     right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
%     abstract brief and self-contained, limiting it to one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the camera-ready
%     phase.

%     \subsection{Partitioning the Text}

%     You should organize your paper into sections and paragraphs to help readers place
%     a structure on the material and understand its contributions.

%     \subsubsection{Sections and Subsections}

%     Section headings should be numbered, flush left, and set in 11~pt bold type
%     with the content words capitalized. Leave 0.25~inches of space before the heading
%     and 0.15~inches after the heading.

%     Similarly, subsection headings should be numbered, flush left, and set in 10~pt
%     bold type with the content words capitalized. Leave 0.2~inches of space before
%     the heading and 0.13~inches afterward.

%     Finally, subsubsection headings should be numbered, flush left, and set in
%     10~pt small caps with the content words capitalized. Leave 0.18~inches of space
%     before the heading and 0.1~inches after the heading.

%     Please use no more than three levels of headings.

%     \subsubsection{Paragraphs and Footnotes}

%     Within each section or subsection, you should further partition the paper into
%     paragraphs. Do not indent the first line of a given paragraph, but insert a blank
%     line between succeeding ones.

%     You can use footnotes\footnote{Footnotes should be complete sentences.} to
%     provide readers with additional information about a topic without
%     interrupting the flow of the paper. Indicate footnotes with a number in the text
%     where the point is most relevant. Place the footnote in 9~point type at the
%     bottom of the column in which it appears. Precede the first footnote in a column
%     with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can appear
%     in each column, in the same order as they appear in the text, but spread
%     them across columns and pages if possible.}

%     \begin{figure}[ht]
%         \vskip 0.2in
%         \begin{center}
%             \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
%             \caption{Historical locations and number of accepted papers for
%             International Machine Learning Conferences (ICML 1993 -- ICML 2008) and
%             International Workshops on Machine Learning (ML 1988 -- ML 1992). At
%             the time this figure was produced, the number of accepted papers for
%             ICML 2008 was unknown and instead estimated.}
%             \label{icml-historical}
%         \end{center}
%         \vskip -0.2in
%     \end{figure}

%     \subsection{Figures}

%     You may want to include figures in the paper to illustrate your approach and
%     results. Such artwork should be centered, legible, and separated from the text.
%     Lines should be dark and at least 0.5~points thick for purposes of
%     reproduction, and text should not appear on a gray background.

%     Label all distinct components of each figure. If the figure takes the form
%     of a graph, then give a name for each axis and include a legend that briefly
%     describes each curve. Do not include a title inside the figure; instead, the
%     caption should serve this function.

%     Number figures sequentially, placing the figure number and caption \emph{after}
%     the graphics, with at least 0.1~inches of space before the caption and 0.1~inches
%     after it, as in \cref{icml-historical}. The figure caption should be set in 9~point
%     type and centered unless it runs two or more lines, in which case it should
%     be flush left. You may float figures to the top or bottom of a column, and
%     you may set wide figures across both columns (use the environment \texttt{figure*}
%     in \LaTeX). Always place two-column figures at the top or bottom of the page.

%     \subsection{Algorithms}

%     If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
%     environments to format pseudocode. These require the corresponding stylefiles,
%     algorithm.sty and algorithmic.sty, which are supplied with this package.
%     \cref{alg:example} shows an example.

%     \begin{algorithm}
%         [tb]
%         \caption{Bubble Sort}
%         \label{alg:example}
%         \begin{algorithmic}
%             \STATE {\bfseries Input:} data $x_{i}$, size $m$ \REPEAT \STATE
%             Initialize $noChange = true$. \FOR{$i=1$ {\bfseries to} $m-1$} \IF{$x_{i}> x_{i+1}$}
%             \STATE Swap $x_{i}$ and $x_{i+1}$ \STATE $noChange = false$ \ENDIF \ENDFOR
%             \UNTIL{$noChange$ is $true$}
%         \end{algorithmic}
%     \end{algorithm}

%     \subsection{Tables}

%     You may also want to include tables that summarize material. Like figures,
%     these should be centered, legible, and numbered consecutively. However,
%     place the title \emph{above} the table with at least 0.1~inches of space before
%     the title and the same after it, as in \cref{sample-table}. The table title should
%     be set in 9~point type and centered unless it runs two or more lines, in
%     which case it should be flush left.

%     % Note use of \abovespace and \belowspace to get reasonable spacing
%     % above and below tabular lines.

%     \begin{table}[t]
%         \caption{Classification accuracies for naive Bayes and flexible Bayes on
%         various data sets.}
%         \label{sample-table} \vskip 0.15in
%         \begin{center}
%             \begin{small}
%                 \begin{sc}
%                     \begin{tabular}{lcccr}
%                         \toprule Data set & Naive         & Flexible      & Better?  \\
%                         \midrule Breast   & 95.9$\pm$ 0.2 & 96.7$\pm$ 0.2 & $\surd$  \\
%                         Cleveland         & 83.3$\pm$ 0.6 & 80.0$\pm$ 0.6 & $\times$ \\
%                         Glass2            & 61.9$\pm$ 1.4 & 83.8$\pm$ 0.7 & $\surd$  \\
%                         Credit            & 74.8$\pm$ 0.5 & 78.3$\pm$ 0.6 &          \\
%                         Horse             & 73.3$\pm$ 0.9 & 69.7$\pm$ 1.0 & $\times$ \\
%                         Meta              & 67.1$\pm$ 0.6 & 76.5$\pm$ 0.5 & $\surd$  \\
%                         Pima              & 75.1$\pm$ 0.6 & 73.9$\pm$ 0.5 &          \\
%                         Vehicle           & 44.9$\pm$ 0.6 & 61.5$\pm$ 0.4 & $\surd$  \\
%                         \bottomrule
%                     \end{tabular}
%                 \end{sc}
%             \end{small}
%         \end{center}
%         \vskip -0.1in
%     \end{table}

%     Tables contain textual material, whereas figures contain graphical material.
%     Specify the contents of each row and column in the table's topmost row. Again,
%     you may float tables to a column's top or bottom, and set wide tables across
%     both columns. Place two-column tables at the top or bottom of the page.

%     \subsection{Theorems and such}
%     The preferred way is to number definitions, propositions, lemmas, etc.
%     consecutively, within sections, as shown below.
%     \begin{definition}
%         \label{def:inj} A function $f:X \to Y$ is injective if for any $x,y\in X$
%         different, $f(x)\ne f(y)$.
%     \end{definition}
%     Using \cref{def:inj} we immediate get the following result:
%     \begin{proposition}
%         If $f$ is injective mapping a set $X$ to another set $Y$, the
%         cardinality of $Y$ is at least as large as that of $X$
%     \end{proposition}
%     \begin{proof}
%         Left as an exercise to the reader.
%     \end{proof}
%     \cref{lem:usefullemma} stated next will prove to be useful.
%     \begin{lemma}
%         \label{lem:usefullemma} For any $f:X \to Y$ and $g:Y\to Z$ injective functions,
%         $f \circ g$ is injective.
%     \end{lemma}
%     \begin{theorem}
%         \label{thm:bigtheorem} If $f:X\to Y$ is bijective, the cardinality of
%         $X$ and $Y$ are the same.
%     \end{theorem}
%     An easy corollary of \cref{thm:bigtheorem} is the following:
%     \begin{corollary}
%         If $f:X\to Y$ is bijective, the cardinality of $X$ is at least as large
%         as that of $Y$.
%     \end{corollary}
%     \begin{assumption}
%         The set $X$ is finite. \label{ass:xfinite}
%     \end{assumption}
%     \begin{remark}
%         According to some, it is only the finite case (cf. \cref{ass:xfinite}) that
%         is interesting.
%     \end{remark}
%     %restatable

%     \subsection{Citations and References}

%     Please use APA reference format regardless of your formatter or word
%     processor. If you rely on the \LaTeX\/ bibliographic facility, use \texttt{natbib.sty}
%     and \texttt{icml2025.bst} included in the style-file package to obtain this format.

%     Citations within the text should include the authors' last names and year. If
%     the authors' names are included in the sentence, place only the year in parentheses,
%     for example when referencing Arthur Samuel's pioneering work \yrcite{Samuel59}.
%     Otherwise place the entire reference in parentheses with the authors and
%     year separated by a comma \cite{Samuel59}. List multiple references separated
%     by semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.' construct
%     only for citations with three or more authors or after listing all authors to
%     a publication in an earlier reference \cite{MachineLearningI}.

%     Authors should cite their own work in the third person in the initial version
%     of their paper submitted for blind review. Please refer to
%     \cref{author info} for detailed instructions on how to cite your own papers.

%     Use an unnumbered first-level section heading for the references, and use a hanging
%     indent style, with the first line of the reference flush against the left
%     margin and subsequent lines indented by 10 points. The references at the end
%     of this document give examples for journal articles \cite{Samuel59},
%     conference publications \cite{langley00}, book chapters \cite{Newell81},
%     books \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical
%     reports \cite{mitchell80}, and dissertations \cite{kearns89}.

%     Alphabetize references by the surnames of the first authors, with single
%     author entries preceding multiple author entries. Order references for the same
%     authors by year of publication, with the earliest first. Make sure that each
%     reference includes all relevant information (e.g., page numbers).

%     Please put some effort into making references complete, presentable, and consistent,
%     e.g. use the actual current name of authors. If using bibtex, please protect
%     capital letters of names and abbreviations in titles, for example, use \{B\}ayesian
%     or \{L\}ipschitz in your .bib file.

%     \section*{Accessibility}
%     Authors are kindly asked to make their submissions as accessible as possible
%     for everyone including people with disabilities and sensory or neurological
%     differences. Tips of how to achieve this and what to pay attention to will be
%     provided on the conference website \url{http://icml.cc/}.

%     \section*{Software and Data}

%     If a paper is accepted, we strongly encourage the publication of software and
%     data with the camera-ready version of the paper whenever appropriate. This
%     can be done by including a URL in the camera-ready copy. However, \textbf{do
%     not} include URLs that reveal your institution or identity in your submission
%     for review. Instead, provide an anonymous URL or upload the material as ``Supplementary
%     Material'' into the OpenReview reviewing system. Note that reviewers are not
%     required to look at this material when writing their review.

%     % Acknowledgements should only appear in the accepted version.
%     \section*{Acknowledgements}

%     \textbf{Do not} include acknowledgements in the initial version of the paper
%     submitted for blind review.

%     If a paper is accepted, the final camera-ready version can (and usually
%     should) include acknowledgements. Such acknowledgements should be placed at
%     the end of the section, in an unnumbered section that does not count towards
%     the paper page limit. Typically, this will include thanks to reviewers who gave
%     useful comments, to colleagues who contributed to the ideas, and to funding agencies
%     and corporate sponsors that provided financial support.

%     \section*{Impact Statement}

%     Authors are \textbf{required} to include a statement of the potential broader
%     impact of their work, including its ethical aspects and future societal
%     consequences. This statement should be in an unnumbered section at the end of
%     the paper (co-located with Acknowledgements -- the two may appear in either order,
%     but both must be before References), and does not count toward the paper
%     page limit. In many cases, where the ethical impacts and expected societal
%     implications are those that are well established when advancing the field of
%     Machine Learning, substantial discussion is not required, and a simple
%     statement such as the following will suffice:

%     ``This paper presents work whose goal is to advance the field of Machine
%     Learning. There are many potential societal consequences of our work, none
%     which we feel must be specifically highlighted here.''

%     The above statement can be used verbatim in such cases, but we encourage authors
%     to think about whether there is content which does warrant further discussion,
%     as this statement will be apparent if the paper is later flagged for ethics
%     review.

    % In the unusual situation where you want a paper to appear in the
    % references without citing it in the main text, use \nocite
    % \nocite{langley00}
    \bibliographystyle{icml2025}
    \bibliography{example_paper}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % APPENDIX
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % \newpage
    % \appendix
    % \onecolumn
    % \section{You \emph{can} have an appendix here.}

    % You can have as much text here as you want. The main body must be at most $8$
    % pages long. For the final version, one more page can be added. If you want, you
    % can use an appendix like this one.

    % The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you
    % prefer a one-column appendix, or can be removed if you prefer a two-column
    % appendix. Apart from this possible change, the style (font size, spacing, margins,
    % page numbering, etc.) should be kept the same as the main body.
    % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.