\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize, noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Comparative Study of DQN-Family Algorithms on Gymnasium Control Tasks}

\begin{document}
    \twocolumn[ \icmltitle{A Comparative Study of DQN-Family Algorithms on Gymnasium Control Tasks}

    \icmlsetsymbol{equal}{*}

    \begin{icmlauthorlist}
        \icmlauthor{Qi Deng}{equal,fudan}
        \icmlauthor{Zicheng Fan}{equal,fudan}
        \icmlauthor{Kaiming Zhan}{equal,fudan}
    \end{icmlauthorlist}

    \icmlaffiliation{fudan}{School of Computing and Intelligence Innovation, Fudan University}

    \icmlcorrespondingauthor{Qi Deng}{25213050008@m.fudan.edu.cn}
    \icmlcorrespondingauthor{Zicheng Fan}{25213050157@m.fudan.edu.cn}
    \icmlcorrespondingauthor{Kaiming Zhan}{25213050555@m.fudan.edu.cn}


    \icmlkeywords{Machine Learning, ICML}

    \vskip 0.3in ]

    \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

    \begin{abstract}
        Deep Q-Networks (DQN) have become a cornerstone of value-based deep reinforcement learning, 
        yet suffer from overestimation bias and inefficient value function representation. 
        This paper presents a comparative empirical study of three DQN-family algorithms—vanilla DQN, Double DQN, and Dueling DQN—on canonical control benchmarks from the Gymnasium suite. 
        We provide detailed algorithmic descriptions with pseudocode for each variant, 
        highlighting their orthogonal design improvements: Double DQN decouples action selection and evaluation to mitigate overestimation, 
        while Dueling DQN decomposes the Q-network into separate value and advantage streams to accelerate learning. 
        Under matched hyperparameters and training protocols on CartPole-v3 and LunarLander-v3 tasks, 
        our experiments demonstrate a consistent performance ranking where Dueling-DQN outperforms Double-DQN, which in turn outperforms vanilla DQN, in terms of stability, sample efficiency, and final return. 
        These results validate the complementary benefits of both improvements and offer practical guidance for practitioners selecting value-based RL methods for discrete action spaces.
        Code is available at \url{https://github.com/Hyrsta/Q-learning}.
    \end{abstract}

    \section{Introduction}
    \label{intro}
    
    Deep reinforcement learning (RL) has achieved remarkable success across diverse domains, 
    from mastering complex games~\cite{mnih2015human} to robotic control~\cite{levine2016end} and autonomous decision-making. 
    At the heart of many of these advances lies value-based learning~\cite{sutton1998reinforcement}, 
    where an agent learns to estimate the long-term expected return (value) of taking actions in given states 
    and derives a policy by acting greedily with respect to these learned values. 
    Among value-based methods, Deep Q-Networks (DQN)~\cite{mnih2015human} represent a landmark breakthrough, 
    combining classical Q-learning with deep neural network function approximation and two key stabilization techniques: 
    experience replay and target networks. 
    Despite its success, vanilla DQN is known to suffer from overestimation bias~\cite{thrun2014issues} in Q-value updates, 
    which can degrade sample efficiency and policy quality. 
    Subsequent variants—Double DQN~\cite{DBLP:journals/corr/HasseltGS15} and Dueling DQN~\cite{DBLP:journals/corr/WangFL15}—address these limitations through orthogonal improvements: 
    Double DQN mitigates overestimation by decoupling action selection and evaluation, 
    while Dueling DQN redesigns the network architecture to separately model state values and action advantages.

    This paper provides a comparative empirical study of these three DQN-family algorithms 
    on canonical control benchmarks from the Gymnasium suite. 
    Our contributions are threefold: 
    (1) we present clear algorithmic descriptions and pseudocode for DQN, Double DQN, and Dueling DQN, 
    highlighting their key differences and design rationales; 
    (2) we evaluate all three methods under matched hyperparameters and training protocols to ensure fair comparison; 
    and (3) we analyze their relative performance in terms of stability, sample efficiency, and final return, 
    offering practical insights into when each variant provides tangible benefits. 
    The remainder of this section describes the three algorithms in detail. 
    Section~\ref{sec:experiments} outlines our experimental setup, 
    Section~\ref{sec:results} presents the results, 
    and Section~\ref{sec:conclusion} concludes with a discussion of the findings.


    \begin{algorithm*}
        [htb]
        \caption{DQN with Experience Replay}
        \label{alg:dqn_replay}
        \begin{algorithmic}
            [1] \STATE {\bfseries Input:} environment Env, replay capacity $N$, batch
            size $B$, discount $\gamma$, target update period $C$, learning rate
            $\alpha$, exploration schedule $\epsilon_{t}$ \STATE Initialize replay
            buffer $\mathcal{D}\leftarrow \{\}$ of capacity $N$ \STATE Initialize
            online Q-network $Q(s,a;\theta)$ with random weights $\theta$ \STATE
            Initialize target network weights $\theta^{-}\leftarrow \theta$ \FOR{episode = 1 \textbf{to} M}
            \STATE $s \leftarrow$ Env.reset() \FOR{t = 1 \textbf{to} T} \STATE With
            probability $\epsilon_{t}$ select a random action $a$, otherwise
            $a \leftarrow \arg\max_{a'}Q(s,a';\theta)$ \STATE Execute $a$, observe
            reward $r$ and next state $s'$ \STATE Store transition $(s,a,r,s')$ in
            $\mathcal{D}$ (drop oldest if full) \IF{ $|\mathcal{D}| \ge B$ }
            \STATE Sample random batch
            $\{(s_{j},a_{j},r_{j},s'_{j})\}_{j=1}^{B}$ from $\mathcal{D}$ \STATE
            For each sample compute target:
            \[
                y_{j}=
                \begin{cases}
                    r_{j}                                          & \text{if }s'_{j}\text{ is terminal} \\
                    r_{j}+ \gamma \max_{a'}Q(s'_{j},a';\theta^{-}) & \text{otherwise}
                \end{cases}
            \]
            \STATE Perform a gradient step on loss $\mathcal{L}(\theta)=\frac{1}{B}
            \sum_{j}(y_{j}- Q(s_{j},a_{j};\theta))^{2}$ \ENDIF \STATE Every $C$ steps:
            $\theta^{-}\leftarrow \theta$ \STATE $s \leftarrow s'$ \ENDFOR
            \ENDFOR
        \end{algorithmic}
    \end{algorithm*}


    \subsection{Deep Q-Network with Experience Replay}

    Q-learning is one of the earliest and most classical value-based methods in Reinforcement Learning (RL), 
    originally proposed by Watkins and Dayan~\cite{watkins1992qlearning}. 
    It enables an agent to learn an optimal policy through trial-and-error interactions with the environment 
    without requiring prior knowledge of the environment's transition probabilities. 
    The algorithm is formulated under the framework of a Markov Decision Process (MDP)~\cite{sutton1998reinforcement}, 
    where decision-making scenarios are represented by states ($s$) and actions ($a$).



    Algorithm~\ref{alg:dqn_replay} summarizes the core training loop of DQN with experience replay. 
    The agent interacts with the environment by selecting actions via an $\varepsilon$-greedy policy: 
    with probability $\varepsilon_t$ it explores randomly, otherwise it exploits the current Q-network by choosing $\arg\max_{a'} Q(s,a';\theta)$. 
    Each observed transition $(s,a,r,s')$ is stored in a replay buffer $\mathcal{D}$ of fixed capacity $N$; 
    when the buffer is full, the oldest transitions are discarded. 
    Once sufficient data has been collected ($|\mathcal{D}| \ge B$), 
    the algorithm samples a mini-batch of transitions uniformly at random and computes target values $y_j$ using a separate target network with frozen parameters $\theta^{-}$. 
    The online network parameters $\theta$ are then updated by minimizing the mean-squared temporal-difference error. 
    Periodically (every $C$ steps), the target network is synchronized with the online network to ensure stable bootstrapping. 
    This decoupling of action selection and target evaluation, combined with experience replay to break temporal correlations, 
    enables DQN to learn robust value functions even in complex, high-dimensional state spaces.

    \subsection{Double Deep Q-Network}

    Double DQN~\cite{DBLP:journals/corr/HasseltGS15} reduces the overestimation bias of the original DQN by decoupling
    the action selection and action evaluation between the online network and
    the target network, building upon the earlier Double Q-learning algorithm~\cite{hasselt2010double}. The pseudocode below shows this modification to the target
    computation while keeping the overall training loop and replay buffer logic identical
    to standard DQN.

    \begin{algorithm*}
        [tb]
        \caption{Double DQN with Experience Replay}
        \label{alg:double_dqn}
        \begin{algorithmic}
            [1] \STATE {\bfseries Input:} environment Env, replay capacity $N$, batch
            size $B$, discount $\gamma$, target update period $C$, learning rate
            $\alpha$, exploration schedule $\epsilon_{t}$ \STATE Initialize replay
            buffer $\mathcal{D}\leftarrow \{\}$ of capacity $N$ \STATE Initialize
            online Q-network $Q(s,a;\theta)$ with random weights $\theta$ \STATE
            Initialize target network weights $\theta^{-}\leftarrow \theta$ \FOR{episode = 1 \textbf{to} M}
            \STATE $s \leftarrow$ Env.reset() \FOR{t = 1 \textbf{to} T} \STATE With
            probability $\epsilon_{t}$ select a random action $a$, otherwise
            $a \leftarrow \arg\max_{a'}Q(s,a';\theta)$ \STATE Execute $a$, observe
            reward $r$ and next state $s'$ \STATE Store transition $(s,a,r,s')$ in
            $\mathcal{D}$ (drop oldest if full) \IF{ $|\mathcal{D}| \ge B$ }
            \STATE Sample random batch
            $\{(s_{j},a_{j},r_{j},s'_{j})\}_{j=1}^{B}$ from $\mathcal{D}$ \STATE
            For each sample compute Double DQN target:
            \[
                y_{j}=
                \begin{cases}
                    r_{j}                                                                            & \text{if }s'_{j}\text{ is terminal} \\
                    r_{j}+ \gamma\; Q\bigl(s'_{j}, \arg\max_{a}Q(s'_{j},a;\theta) ; \theta^{-}\bigr) & \text{otherwise}
                \end{cases}
            \]
            \STATE Perform a gradient step on loss $\mathcal{L}(\theta)=\frac{1}{B}
            \sum_{j}(y_{j}- Q(s_{j},a_{j};\theta))^{2}$ \ENDIF \STATE Every $C$ steps:
            $\theta^{-}\leftarrow \theta$ \STATE $s \leftarrow s'$ \ENDFOR
            \ENDFOR
        \end{algorithmic}
    \end{algorithm*}

    Algorithm~\ref{alg:double_dqn} presents the Double DQN variant, which addresses the overestimation bias inherent in standard DQN. 
    The key difference lies in the target computation: 
    whereas vanilla DQN uses $\max_{a'} Q(s'_{j}, a'; \theta^{-})$ to both select and evaluate the best action, 
    Double DQN decouples these two operations. 
    Specifically, the online network $Q(s, a; \theta)$ is used to select the greedy action $\arg\max_{a} Q(s'_{j}, a; \theta)$, 
    while the target network $Q(s, a; \theta^{-})$ evaluates the value of that selected action. 
    This decomposition prevents the max operator from consistently picking overestimated values, 
    as the selection and evaluation are performed by different networks with different parameterizations. 
    All other components—experience replay, $\varepsilon$-greedy exploration, and periodic target network updates—remain identical to standard DQN. 
    Empirically, this simple modification yields more accurate Q-value estimates and improved sample efficiency, 
    particularly in environments where overestimation can lead to suboptimal policies or training instabilities.


    \subsection{Dueling Deep Q-Network}

    Dueling DQN~\cite{DBLP:journals/corr/WangFL15} separates the representation of state-value and advantage for
    each action, which helps the agent learn which states are (or are not) valuable
    without having to learn the effect of each action for every state. The
    architecture splits the final layers of the Q-network into two streams that
    estimate a scalar state-value $V(s)$ and an advantage vector $A(s,a)$; these
    are combined to produce Q-values via $Q(s,a)=V(s) + A(s,a) - \frac{1}{|\mathcal{A}|}
    \sum_{a'}A(s,a')$.

    The pseudocode below shows a dueling variant that uses experience replay and
    a target network; it otherwise follows the same training loop as the DQN
    family (optionally usable together with Double DQN selection/evaluation).

    \begin{algorithm*}
        [tb]
        \caption{Dueling DQN with Experience Replay}
        \label{alg:dueling_dqn}
        \begin{algorithmic}
            [1] \STATE {\bfseries Input:} environment Env, replay capacity $N$, batch
            size $B$, discount $\gamma$, target update period $C$, learning rate
            $\alpha$, exploration schedule $\epsilon_{t}$ \STATE Initialize replay
            buffer $\mathcal{D}\leftarrow \{\}$ of capacity $N$ \STATE Initialize
            online dueling Q-network: shared trunk; value head $V(s;\theta)$;
            advantage head $A(s,a;\theta)$; combine to $Q(s,a;\theta)$ via advantage
            normalization \STATE Initialize target network weights $\theta^{-}\leftarrow
            \theta$ \FOR{episode = 1 \textbf{to} M} \STATE $s \leftarrow$ Env.reset()
            \FOR{t = 1 \textbf{to} T} \STATE With probability $\epsilon_{t}$ select
            a random action $a$, otherwise
            $a \leftarrow \arg\max_{a'}Q(s,a';\theta)$ \STATE Execute $a$, observe
            reward $r$ and next state $s'$ \STATE Store transition $(s,a,r,s')$ in
            $\mathcal{D}$ (drop oldest if full) \IF{ $|\mathcal{D}| \ge B$ }
            \STATE Sample random batch
            $\{(s_{j},a_{j},r_{j},s'_{j})\}_{j=1}^{B}$ from $\mathcal{D}$ \STATE
            For each sample compute target $y_{j}$ (use either DQN or Double DQN
            style target):
            \[
                y_{j} =
                \begin{cases}
                    r_{j}                                           & \text{if }s'_{j} \text{ is terminal} \\
                    r_{j} + \gamma \max_{a'}Q(s'_{j},a';\theta^{-}) & \text{otherwise}
                \end{cases}
            \]
            \STATE Note: when combining with Double DQN, replace the $\max$ evaluation
            by
            \[
                r_{j} + \gamma\; Q\bigl(s'_{j}, \arg\max_{a} Q(s'_{j},a;\theta) ;
                \theta^{-} \bigr)
            \]
            \STATE Perform a gradient step on loss $\mathcal{L}(\theta)=\frac{1}{B}
            \sum_{j} (y_{j} - Q(s_{j},a_{j};\theta))^{2}$; gradients
            backpropagate through both value and advantage heads \ENDIF \STATE
            Every $C$ steps: $\theta^{-} \leftarrow \theta$ \STATE $s \leftarrow
            s'$ \ENDFOR \ENDFOR
        \end{algorithmic}
    \end{algorithm*}

    Algorithm~\ref{alg:dueling_dqn} introduces the Dueling DQN architecture, which represents a complementary improvement orthogonal to Double DQN's target-computation fix. 
    Unlike the previous variants that modify the learning target, Dueling DQN redesigns the network architecture itself. 
    The key innovation is the decomposition of the Q-network into two parallel streams: 
    a value stream that estimates the state-value function $V(s;\theta)$ (measuring how good it is to be in a given state), 
    and an advantage stream that estimates the advantage function $A(s,a;\theta)$ (measuring the relative benefit of each action over the average). 
    These streams share a common convolutional or fully-connected trunk for feature extraction and are recombined via 
    $Q(s,a;\theta) = V(s;\theta) + A(s,a;\theta) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s,a';\theta)$, 
    where the mean-subtraction normalization ensures identifiability. 
    This architectural change enables the network to learn which states are inherently valuable independently of the action-conditioned advantages, 
    accelerating learning in scenarios where many actions yield similar outcomes. 
    Importantly, the dueling architecture is compatible with both standard DQN and Double DQN target computations, 
    as shown in the algorithm pseudocode: one can use either the vanilla max-based target or the Double DQN decoupled target. 
    The gradient updates backpropagate through both value and advantage heads, jointly optimizing the shared representation. 
    This modularity allows Dueling DQN to combine with Double DQN for cumulative benefits—mitigating overestimation while improving generalization across actions.


    \section{Experimental Setup}
    \label{sec:experiments}

    \subsection{Implementation Details}
    All experiments were conducted on a single workstation equipped with an Intel Xeon Gold 6326 CPU (2.90 GHz), an NVIDIA RTX A6000 GPU (48 GB VRAM), and Ubuntu 20.04.4 LTS. 
    To ensure reproducibility and eliminate potential performance interference, we ran all training jobs in isolation with no concurrent GPU workloads. 
    Our implementation builds upon PyTorch~\cite{paszke2019pytorch} and uses the Gymnasium API~\cite{towers2024gymnasium} (version 0.29.1) for environment interfacing.

    \subsection{Task Environments}
    We evaluate on two canonical control benchmarks using the Gymnasium API with ale-py:
    \paragraph{CartPole-v3.}
    A classic control task with a 4-dimensional continuous observation vector and a discrete 2-action space (push left/right). Episodes terminate or truncate according to the environment’s default criteria; all default physics and termination settings are retained.
    \paragraph{LunarLander-v3 (discrete).}
    A 2-D lander with an 8-dimensional continuous observation vector and a discrete 4-action space (do nothing; left/right engine; main engine). Episodes terminate upon crash or successful landing; time-limit truncation follows the environment’s default. We keep all default reward shaping and environment parameters in both tasks.

    \subsection{Training Configuration}
    We train value-based agents from the DQN family with standard experience replay and a target network. For the loss, we adopt the Huber loss in all settings, as it is empirically more robust to outliers and stabilizes Q-learning updates relative to mean-squared error. To ensure reproducibility and comparability, hyperparameters follow widely used community baselines, varying only where task-specific totals or memory/mini-batch sizes are customary. Exploration is $\varepsilon$-greedy with linear decay from the initial to the final value over the specified exploration fraction of total training steps.

    \begin{table}[t]
        \centering
        \caption{Training hyperparameters. CartPole-v3 and LunarLander-v3 (discrete) follow community baseline configurations; Huber loss is used throughout.}
        \label{tab:hyperparams}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{@{}lcc@{}}
            \toprule
            \textbf{Hyperparameter} & \textbf{CartPole-v3} & \textbf{LunarLander-v3 (discrete)} \\
            \midrule
            \texttt{total\_timesteps}          & 100000 & 500000 \\
            \texttt{learning\_rate}            & $5\times 10^{-4}$ & $5\times 10^{-4}$ \\
            \texttt{buffer\_size}              & 50000  & 200000 \\
            \texttt{batch\_size}               & 64     & 128 \\
            \texttt{gamma}                     & 0.99   & 0.99 \\
            \texttt{train\_frequency}          & 1      & 4 \\
            \texttt{gradient\_steps}           & 1      & 1 \\
            \texttt{target\_update\_interval}  & 1000   & 4000 \\
            \texttt{target\_update\_tau}       & 1.0    & 1.0 \\
            \texttt{learning\_starts}          & 1000   & 10000 \\
            \texttt{exploration\_fraction}     & 0.20   & 0.40 \\
            \texttt{exploration\_initial\_eps} & 1.00   & 1.00 \\
            \texttt{exploration\_final\_eps}   & 0.05   & 0.02 \\
            \texttt{loss}                      & Huber  & Huber \\
            \bottomrule
        \end{tabular}}
    \end{table}

    \subsection{Evaluation Protocol}
    We report learning curves as the episodic return obtained from deterministic evaluation rollouts ($\epsilon = 0$) at fixed training intervals. At each interval, we execute an evaluation rollout with the current policy and record its episodic return; the curve is the sequence of these values over training. No smoothing is applied and all results are from a single random seed unless stated otherwise.

    \begin{figure*}[th]
        \centering
        % Compiles even if the file doesn't exist; replace with your actual figure later.
        \IfFileExists{figures/learning_curves.png}{%
            \includegraphics[width=0.85\linewidth]{figures/learning_curves.png}%
        }{%
            \fbox{\parbox{0.85\linewidth}{\centering Placeholder: learning\_curves.pdf not found.\\
            Put your plot at \texttt{figures/learning\_curves.pdf}.}}%
        }
        \caption{Learning curves on CartPole-v3 and LunarLander-v3 (discrete). Shaded regions indicate variability across random seeds.}
        \label{fig:learning_curves}
    \end{figure*}


    \section{Results}
    \label{sec:results}
    Figure~\ref{fig:learning_curves} shows the learning curve across both tasks. Under identical hyperparameters, our runs exhibit a consistent ranking $\text{DQN} < \text{Double-DQN} < \text{Dueling-DQN}$: DQN improves but displays noticeable oscillations and occasional regressions; Double-DQN dampens these fluctuations and is more sample-efficient; Dueling-DQN accelerates early learning and achieves the highest plateau with the most stable trajectory. The gap is most pronounced in the harder phases of training, where the variants sustain steadier progress toward convergence. For qualitative results (videos) from the optimized models for each algorithm and environment, please refer to the project repository: https://github.com/Hyrsta/Q-learning
    
    \section{Conclusion}
    \label{sec:conclusion}
    We presented a systematic empirical comparison of three DQN-family algorithms—vanilla DQN, Double DQN, and Dueling DQN—on two canonical control tasks from the Gymnasium suite. 
    Under identical hyperparameters and training protocols, our experiments reveal a consistent performance ordering: Dueling DQN outperforms Double DQN, which in turn surpasses vanilla DQN, across metrics of stability, sample efficiency, and final return.
    
    These results provide empirical validation for the theoretical motivations underlying each algorithmic improvement. 
    Double DQN's decoupled action selection and evaluation successfully mitigates the overestimation bias inherent in standard Q-learning, 
    while Dueling DQN's architectural decomposition of state values and action advantages enables more efficient generalization across the action space. 
    Importantly, our controlled experimental setup—using matched optimizers, Huber loss, and community-standard hyperparameters—ensures that observed performance gains reflect the core algorithmic innovations rather than confounding implementation details.
    
    Future work could extend this analysis in several directions: 
    evaluating the combined Dueling Double DQN architecture to assess whether the improvements compose additively; 
    testing on more challenging domains with higher-dimensional observation spaces or larger action sets; 
    incorporating additional improvements such as prioritized experience replay~\cite{schaul2015prioritized} or the full Rainbow~\cite{hessel2018rainbow} combination; 
    and investigating the sensitivity of these rankings to hyperparameter choices and random seed variation through comprehensive ablation studies.




    \bibliographystyle{icml2025}
    \bibliography{example_paper}


\end{document}

